{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Excel Sheet to a Single Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Excel file\n",
    "excel_data=pd.ExcelFile(\"Bird_Monitoring_Data_FOREST.XLSX\")\n",
    "\n",
    "#There are multiple sheets inside the excel, we need to combine that together\n",
    "\n",
    "sheet_names=excel_data.sheet_names #Stores sheet names as a list\n",
    "\n",
    "#Converting Sheet names and corresponding data into Key:Value pairs in a dictionary\n",
    "sheets_dict={sheet :excel_data.parse(sheet) for sheet in sheet_names}\n",
    "\n",
    "#Loops through rows to add data source(Corresponding sheet) and combines the dataset by stacking vertically\n",
    "combine_df=pd.concat(\n",
    "    [df.assign(sheet=sheet_name) for sheet_name,df in sheets_dict.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "#removes the source column as an equivalent column already exists and stores the data into a dataframe\n",
    "df = combine_df.drop(columns=['sheet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8546, 29)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape # To identify the data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 8546 rows and 29 columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Admin_Unit_Code                   0\n",
       "Sub_Unit_Code                  7824\n",
       "Site_Name                         0\n",
       "Plot_Name                         0\n",
       "Location_Type                     0\n",
       "Year                              0\n",
       "Date                              0\n",
       "Start_Time                        0\n",
       "End_Time                          0\n",
       "Observer                          0\n",
       "Visit                             0\n",
       "Interval_Length                   0\n",
       "ID_Method                         1\n",
       "Distance                         92\n",
       "Flyover_Observed                  0\n",
       "Sex                            5183\n",
       "Common_Name                       0\n",
       "Scientific_Name                   0\n",
       "AcceptedTSN                       9\n",
       "NPSTaxonCode                      0\n",
       "AOU_Code                          0\n",
       "PIF_Watchlist_Status              0\n",
       "Regional_Stewardship_Status       0\n",
       "Temperature                       0\n",
       "Humidity                          0\n",
       "Sky                               0\n",
       "Wind                              0\n",
       "Disturbance                       0\n",
       "Initial_Three_Min_Cnt             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the NULLs distribution in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find columns with more than 70% nulls\n",
    "null_cols = df.columns[df.isnull().sum() > (8546*0.7)]\n",
    "len(null_cols) # No.of columns with more than 70% NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8546, 28)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop them from the DataFrame\n",
    "df = df.drop(columns=null_cols)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store columns containing NULLs\n",
    "null_col = df.columns[df.isnull().sum() > 0]\n",
    "\n",
    "# Impute the NULLs with Mode value of each column\n",
    "for col in null_col:\n",
    "    df[col].fillna(df[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Admin_Unit_Code                0\n",
       "Site_Name                      0\n",
       "Plot_Name                      0\n",
       "Location_Type                  0\n",
       "Year                           0\n",
       "Date                           0\n",
       "Start_Time                     0\n",
       "End_Time                       0\n",
       "Observer                       0\n",
       "Visit                          0\n",
       "Interval_Length                0\n",
       "ID_Method                      0\n",
       "Distance                       0\n",
       "Flyover_Observed               0\n",
       "Sex                            0\n",
       "Common_Name                    0\n",
       "Scientific_Name                0\n",
       "AcceptedTSN                    0\n",
       "NPSTaxonCode                   0\n",
       "AOU_Code                       0\n",
       "PIF_Watchlist_Status           0\n",
       "Regional_Stewardship_Status    0\n",
       "Temperature                    0\n",
       "Humidity                       0\n",
       "Sky                            0\n",
       "Wind                           0\n",
       "Disturbance                    0\n",
       "Initial_Three_Min_Cnt          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the NULLs distribution in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the Duplicates \n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8542, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grass Land"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Excel Sheet to a Single Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from Excel file\n",
    "excel_data2=pd.ExcelFile(\"Bird_Monitoring_Data_GRASSLAND.XLSX\")\n",
    "\n",
    "#There are multiple sheets inside the excel, we need to combine that together\n",
    "\n",
    "sheet_names2=excel_data2.sheet_names #Stores sheet names as a list\n",
    "\n",
    "#Converting Sheet names and corresponding data into Key:Value pairs in a dictionary\n",
    "sheets_dict2={sheet :excel_data2.parse(sheet) for sheet in sheet_names2}\n",
    "\n",
    "#Loops through rows to add data source(Corresponding sheet) and combines the dataset by stacking vertically\n",
    "combine_df2=pd.concat(\n",
    "    [df2.assign(sheet=sheet_name2) for sheet_name2,df2 in sheets_dict2.items()],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "#removes the source column as an equivalent column already exists and stores the data into a dataframe\n",
    "df2 = combine_df2.drop(columns=['sheet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8531, 29)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape # To identify the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Admin_Unit_Code                   0\n",
       "Sub_Unit_Code                  8531\n",
       "Plot_Name                         0\n",
       "Location_Type                     0\n",
       "Year                              0\n",
       "Date                              0\n",
       "Start_Time                        0\n",
       "End_Time                          0\n",
       "Observer                          0\n",
       "Visit                             0\n",
       "Interval_Length                   0\n",
       "ID_Method                         1\n",
       "Distance                       1394\n",
       "Flyover_Observed                  0\n",
       "Sex                               0\n",
       "Common_Name                       0\n",
       "Scientific_Name                   0\n",
       "AcceptedTSN                      24\n",
       "TaxonCode                         2\n",
       "AOU_Code                          0\n",
       "PIF_Watchlist_Status              0\n",
       "Regional_Stewardship_Status       0\n",
       "Temperature                       0\n",
       "Humidity                          0\n",
       "Sky                               0\n",
       "Wind                              0\n",
       "Disturbance                       0\n",
       "Previously_Obs                    0\n",
       "Initial_Three_Min_Cnt             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the NULLs distribution in each column\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find columns with more than 70% nulls\n",
    "null_cols2 = df2.columns[df2.isnull().sum() > (8546*0.7)]\n",
    "len(null_cols2) # No.of columns with more than 70% NULLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8531, 28)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop them from the DataFrame\n",
    "df2 = df2.drop(columns=null_cols2)\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store columns containing NULLs\n",
    "null_col2 = df2.columns[df2.isnull().sum() > 0]\n",
    "\n",
    "# Impute the NULLs with Mode value of each column\n",
    "for col in null_col2:\n",
    "    df2[col].fillna(df2[col].mode()[0], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Admin_Unit_Code                0\n",
       "Plot_Name                      0\n",
       "Location_Type                  0\n",
       "Year                           0\n",
       "Date                           0\n",
       "Start_Time                     0\n",
       "End_Time                       0\n",
       "Observer                       0\n",
       "Visit                          0\n",
       "Interval_Length                0\n",
       "ID_Method                      0\n",
       "Distance                       0\n",
       "Flyover_Observed               0\n",
       "Sex                            0\n",
       "Common_Name                    0\n",
       "Scientific_Name                0\n",
       "AcceptedTSN                    0\n",
       "TaxonCode                      0\n",
       "AOU_Code                       0\n",
       "PIF_Watchlist_Status           0\n",
       "Regional_Stewardship_Status    0\n",
       "Temperature                    0\n",
       "Humidity                       0\n",
       "Sky                            0\n",
       "Wind                           0\n",
       "Disturbance                    0\n",
       "Previously_Obs                 0\n",
       "Initial_Three_Min_Cnt          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the NULLs distribution in each column\n",
    "df2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1705"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicates\n",
    "df2.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the Duplicates \n",
    "df2 = df2.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6826, 28)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOU_Code                       108\n",
       "AcceptedTSN                    107\n",
       "Admin_Unit_Code                 11\n",
       "Common_Name                    108\n",
       "Date                            57\n",
       "Distance                         2\n",
       "Disturbance                      4\n",
       "End_Time                       283\n",
       "Flyover_Observed                 2\n",
       "Humidity                       302\n",
       "ID_Method                        3\n",
       "Initial_Three_Min_Cnt            2\n",
       "Interval_Length                  4\n",
       "Location_Type                    1\n",
       "NPSTaxonCode                   108\n",
       "Observer                         3\n",
       "PIF_Watchlist_Status             2\n",
       "Plot_Name                      408\n",
       "Regional_Stewardship_Status      2\n",
       "Scientific_Name                108\n",
       "Sex                              2\n",
       "Site_Name                       70\n",
       "Sky                              5\n",
       "Start_Time                     282\n",
       "Temperature                    160\n",
       "Visit                            2\n",
       "Wind                             4\n",
       "Year                             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AOU_Code                       107\n",
       "AcceptedTSN                    106\n",
       "Admin_Unit_Code                  4\n",
       "Common_Name                    107\n",
       "Date                            26\n",
       "Distance                         2\n",
       "Disturbance                      4\n",
       "End_Time                       276\n",
       "Flyover_Observed                 2\n",
       "Humidity                       325\n",
       "ID_Method                        3\n",
       "Initial_Three_Min_Cnt            2\n",
       "Interval_Length                  4\n",
       "Location_Type                    1\n",
       "Observer                         3\n",
       "PIF_Watchlist_Status             2\n",
       "Plot_Name                      201\n",
       "Previously_Obs                   1\n",
       "Regional_Stewardship_Status      2\n",
       "Scientific_Name                107\n",
       "Sex                              3\n",
       "Sky                              5\n",
       "Start_Time                     273\n",
       "TaxonCode                      105\n",
       "Temperature                    188\n",
       "Visit                            3\n",
       "Wind                             4\n",
       "Year                             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.nunique().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see mots of the columns are common in both. So let's get rid of the non-common columns and combine the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get common columns\n",
    "common_cols = df.columns.intersection(df2.columns)\n",
    "\n",
    "# Keep only those columns in both DataFrames\n",
    "df = df[common_cols]\n",
    "df2 = df2[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both the datasets\n",
    "\n",
    "combined_df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Admin_Unit_Code                0\n",
       "Plot_Name                      0\n",
       "Location_Type                  0\n",
       "Year                           0\n",
       "Date                           0\n",
       "Start_Time                     0\n",
       "End_Time                       0\n",
       "Observer                       0\n",
       "Visit                          0\n",
       "Interval_Length                0\n",
       "ID_Method                      0\n",
       "Distance                       0\n",
       "Flyover_Observed               0\n",
       "Sex                            0\n",
       "Common_Name                    0\n",
       "Scientific_Name                0\n",
       "AcceptedTSN                    0\n",
       "AOU_Code                       0\n",
       "PIF_Watchlist_Status           0\n",
       "Regional_Stewardship_Status    0\n",
       "Temperature                    0\n",
       "Humidity                       0\n",
       "Sky                            0\n",
       "Wind                           0\n",
       "Disturbance                    0\n",
       "Initial_Three_Min_Cnt          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for NULLs\n",
    "combined_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Duplicates\n",
    "\n",
    "combined_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Combined data for further analysis using Power BI\n",
    "\n",
    "combined_df.to_csv(\"BirdsData.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\">-- End of Data Cleaning --</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
